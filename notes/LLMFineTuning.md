## Model evaluation

### ROUGE (Recall Oriented Understudy for Gisty Evaluation)

Comparison of LLM generated output with reference or set of references (human generated) for text sumnmarization.

**ROUGE-1**

$$
\text{ROUGE-1 Recall} = \frac{\text{number of unigram matches}}{\text{number of unigrams in reference}}
$$

$$
\text{ROUGE-1 Precision} = \frac{\text{number of unigram matches}}{\text{number of unigrams in output}}
$$

$$
\text{ROUGE-1 F1} = {2} \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}
$$

**ROUGE - 2** : It compares bigrams of output with reference. Replace unigrams with bigrams in above formula

**ROUGE - L**: It compares LCS (Largest common subsequence) of output with reference.

$$
\text{ROUGE-L Recall} = \frac{\text{LCS(output, reference)}}{\text{number of unigrams in reference}}
$$

$$
\text{ROUGE-L Precision} = \frac{\text{LCS(output, reference)}}{\text{number of unigrams in output}}
$$

### BLEU (Bilingual Evaluation Understudy)
$$
\text{BLEU} = Avg(\text{precision across n-gram sizes})
$$
This focusses on precision hence it is more suitable for machine translation.

### Benchmarks

Benchmarks evaluate performance of LLM using standardized tests which includes predefined datasets, tasks and scoring mechanisms.

3 step process: Identify dataset across multiple tasks, Testing to run all desired tasks, Evaluation for scoring mechanism.

**HELM (Holistic evaluation of language models)**

Research paper: https://arxiv.org/pdf/2211.09110

**Key takeaways:**
1. Use multi metric approach for evaluation. This includes fairness, bias, toxicity, etc.
2. It has broad coverage of capabilities and risks. 
3. HELM has a top down approach which starts by defining scenarios and metrics required to be validated and then identifying corresponding dataset to run the test case.

## Instruction fine tuning
It is supervised mechanism of
fine-tuning LLMs using a labeled dataset of varied instruction-following tasks. It has prompt, completion pairs for fine tuning weights of the parameters.

## Parameter efficient fine tuninig (PEFT)

Reference: https://www.ibm.com/think/topics/parameter-efficient-fine-tuning

PEFT works by freezing most of pretrained parameters while adding/re-training few trainable parameters.

**Benefits:**
1. Reduces storage cost for fine tuning as the output is not a new model with same size.
2. No catastrophic forgetting as most of the parameters are frozen.
3. Lower compute needs than full retraining.

### LORA (Low-rank adaptation)

**What is low rank matrix?**
**Rank** of the matrix is the number of independent rows or columns in the matrix.

A low rank matrix is a matrix with rank lower than actual matrix.

$$
A =
\begin{bmatrix}
1 & 2 & 3\\[4pt]
2 & 4 & 6\\[4pt]
3 & 6 & 9
\end{bmatrix}
$$

This matrix is rank 1 because every row (and column) is a scalar multiple of the first row. It admits an exact low‑rank factorization as an outer product:

$$
A =
\begin{bmatrix}1\\[4pt]2\\[4pt]3\end{bmatrix}
\begin{bmatrix}1 & 2 & 3\end{bmatrix}
= u\,v^{T},
\quad u=\begin{bmatrix}1\\2\\3\end{bmatrix},\; v=\begin{bmatrix}1\\2\\3\end{bmatrix}.
$$

Therefore a rank‑1 approximation (r = 1) is exact for this matrix.

LoRA breaks down a large matrix representing the weights of the model with smaller low rank matrix. Low rank matrix are fine tuned with gradient descent and the dot product is applied to original weights.

$$
W = 1000 \times 100
$$
$$
A = 1000 \times 8 (rank = 8)
$$
$$
B = 8 \times 1000 (rank = 8)
$$

$$
W new = W + A.B 
$$

Reference: https://www.ibm.com/think/topics/lora

https://medium.com/@rachittayal7/my-experiences-with-finetuning-llms-using-lora-b9c90f1839c6

### Prompt tuning: Soft Prompts

Prompt tuning adds additional embeddings to the input vector which is generated by LLM. This is in contrast to Hard prompts or Prompt engineering where additional prompt is attached to the input prompt in human readable format.

Reference: https://www.youtube.com/watch?v=yu27PWzJI_Y


